{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "partial-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "# import random\n",
    "# random.seed(42)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "with open('/Users/faymajidelhassan/Downloads/ASAC_2023/observations.pickle', 'rb') as handle:\n",
    "    obs = pickle.load(handle)\n",
    "with open('/Users/faymajidelhassan/Downloads/ASAC_2023/actions.pickle', 'rb') as handle:\n",
    "    actions = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a9ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ASACRLEnv(gym.Env):\n",
    "    def __init__(self, observations, actions, action_parameter, action_space):\n",
    "        \"\"\"initialize action space, observation space & load data\"\"\"\n",
    "        self.action_parameter = action_parameter\n",
    "        self.action_space = gym.spaces.Discrete(len(action_space))  # Use Discrete action space\n",
    "        self.observations = observations\n",
    "        self.actions = actions\n",
    "        self.index = 0\n",
    "        self.teamindex = 0\n",
    "        \n",
    "        # Calculate observation space bounds\n",
    "        obs_min = np.min(self.observations[self.teamindex], axis=0)\n",
    "        obs_max = np.max(self.observations[self.teamindex], axis=0)\n",
    "\n",
    "        # Ensure that obs_min and obs_max are numpy arrays with the correct data type\n",
    "        obs_min = np.array(obs_min, dtype=np.float32)\n",
    "        obs_max = np.array(obs_max, dtype=np.float32)\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=obs_min,\n",
    "            high=obs_max,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.curr_obs = self.observations[self.teamindex].iloc[self.index]\n",
    "        self.next_obs = self.curr_obs = self.observations[self.teamindex].iloc[self.index + 1]\n",
    "        self.interval = action_space[1] - action_space[0]\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = self.rewardfunc(action)\n",
    "        self.index += 1\n",
    "        cur_obs = self.curr_obs\n",
    "        next_obs = self.next_obs\n",
    "        done = False  # Set done to False by default\n",
    "        if self.index >= len(self.observations[self.teamindex]) - 2:\n",
    "            self.reset()\n",
    "            done = True  # Set done to True when the episode is finished\n",
    "        self.curr_obs = self.observations[self.teamindex].iloc[self.index]\n",
    "        self.next_obs = self.curr_obs = self.observations[self.teamindex].iloc[self.index + 1]\n",
    "        return self.curr_obs, reward, done, {}\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        set index to 0 and increment team index by 1 if greater than 4 go back to 0\n",
    "        \"\"\"\n",
    "        self.teamindex += 1\n",
    "        if self.teamindex >= 5:\n",
    "            self.teamindex = 0\n",
    "        self.index = 0\n",
    "        self.curr_obs = self.observations[self.teamindex].iloc[self.index]\n",
    "        self.next_obs = self.curr_obs = self.observations[self.teamindex].iloc[self.index + 1]\n",
    "        return self.curr_obs\n",
    "\n",
    "    def rewardfunc(self, action):\n",
    "        \n",
    "        # estimated_action = self.estimate_closest_as(self.actions[self.teamindex][self.action_parameter][self.index])\n",
    "        if action == self.estimate_closest_as(self.actions[self.teamindex][self.action_parameter][self.index]):\n",
    "            return 1000\n",
    "        else:\n",
    "            return -1 * abs(action - self.estimate_closest_as(self.actions[self.teamindex][self.action_parameter][self.index]))\n",
    "\n",
    "\n",
    "    def estimate_closest_as(self, value):\n",
    "        try:\n",
    "            closest_action_index = int(value / self.interval)\n",
    "            closest_action_index = min(closest_action_index, self.action_space.n - 1)\n",
    "        except:\n",
    "            closest_action_index = self.action_space.n - 1\n",
    "        return closest_action_index\n",
    "\n",
    "\n",
    "    def resetinit(self):\n",
    "        self.teamindex = 0\n",
    "        self.index = 0\n",
    "        self.curr_obs = self.observations[self.teamindex].iloc[self.index]\n",
    "        self.next_obs = self.curr_obs = self.observations[self.teamindex].iloc[self.index + 1]\n",
    "        return self.curr_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b3287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "available-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_rl_actionspace=np.linspace(0,2000,9)\n",
    "discrete=list(range(len(water_rl_actionspace)))\n",
    "env=ASACRLEnv(obs,actions,\"water_sup_intervals_sp_min\",discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56c0e0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "env = DummyVecEnv([lambda: env])\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./ppo_tensorboard_logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f027a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardX in /Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages (2.6.2)\n",
      "Requirement already satisfied: numpy in /Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages (from tensorboardX) (1.23.5)\n",
      "Requirement already satisfied: packaging in /Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages (from tensorboardX) (22.0)\n",
      "Requirement already satisfied: protobuf in /Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages (from tensorboardX) (4.23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c6c624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/faymajidelhassan/anaconda3/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages/tensorboardX/event_file_writer.py\", line 208, in run\n",
      "    self._record_writer.write_event(data)\n",
      "  File \"/Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages/tensorboardX/event_file_writer.py\", line 58, in write_event\n",
      "    return self._write_serialized_event(event.SerializeToString())\n",
      "  File \"/Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages/tensorboardX/event_file_writer.py\", line 63, in _write_serialized_event\n",
      "    self._py_recordio_writer.write(event_str)\n",
      "  File \"/Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages/tensorboardX/record_writer.py\", line 189, in write\n",
      "    w(data)\n",
      "OSError: [Errno 28] No space left on device\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m obs \u001b[39m=\u001b[39m new_obs\n\u001b[1;32m     55\u001b[0m \u001b[39m# Log information for TensorBoard\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m writer\u001b[39m.\u001b[39;49madd_scalar(\u001b[39m'\u001b[39;49m\u001b[39mChosen Action\u001b[39;49m\u001b[39m'\u001b[39;49m, action, episode)\n\u001b[1;32m     57\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mObserved State\u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39mmean(obs), episode)  \u001b[39m# You can customize how you log the state\u001b[39;00m\n\u001b[1;32m     58\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mReceived Reward\u001b[39m\u001b[39m'\u001b[39m, reward, episode)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorboardX/writer.py:455\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, display_name, summary_description)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput value: \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m is not a scalar\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(scalar_value))\n\u001b[0;32m--> 455\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_file_writer()\u001b[39m.\u001b[39;49madd_summary(\n\u001b[1;32m    456\u001b[0m     scalar(tag, scalar_value, display_name, summary_description), global_step, walltime)\n\u001b[1;32m    457\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_comet_logger()\u001b[39m.\u001b[39mlog_metric(tag, display_name, scalar_value, global_step)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorboardX/writer.py:145\u001b[0m, in \u001b[0;36mFileWriter.add_summary\u001b[0;34m(self, summary, global_step, walltime)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Adds a `Summary` protocol buffer to the event file.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39mThis method wraps the provided summary in an `Event` protocol buffer\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39mand adds it to the event file.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39m    walltime (from time.time())\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m event \u001b[39m=\u001b[39m event_pb2\u001b[39m.\u001b[39mEvent(summary\u001b[39m=\u001b[39msummary)\n\u001b[0;32m--> 145\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_event(event, global_step, walltime)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorboardX/writer.py:130\u001b[0m, in \u001b[0;36mFileWriter.add_event\u001b[0;34m(self, event, step, walltime)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     \u001b[39m# Make sure step is converted from numpy or other formats\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[39m# since protobuf might not convert depending on version\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     event\u001b[39m.\u001b[39mstep \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(step)\n\u001b[0;32m--> 130\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevent_writer\u001b[39m.\u001b[39;49madd_event(event)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorboardX/event_file_writer.py:139\u001b[0m, in \u001b[0;36mEventFileWriter.add_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Adds an event to the event file.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m  event: An `Event` protocol buffer.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closed:\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event_queue\u001b[39m.\u001b[39;49mput(event)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/multiprocessing/queues.py:89\u001b[0m, in \u001b[0;36mQueue.put\u001b[0;34m(self, obj, block, timeout)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closed:\n\u001b[1;32m     88\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQueue \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m is closed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sem\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m     90\u001b[0m     \u001b[39mraise\u001b[39;00m Full\n\u001b[1;32m     92\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_notempty:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3 import A2C\n",
    "from tensorboardX import SummaryWriter  # Import the SummaryWriter for TensorBoard\n",
    "\n",
    "# Create an instance of your custom environment\n",
    "# env = ASACRLEnv(obs, actions, \"assim_sp\", discrete)\n",
    "# Environment settings\n",
    "EPISODES = 1  # Number of episodes\n",
    "# Exploration settings\n",
    "epsilon = 0.2 # not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.99975\n",
    "MIN_EPSILON = 0.15\n",
    "\n",
    "# Stats settings\n",
    "AGGREGATE_STATS_EVERY = 500  # steps\n",
    "SAVE_MODEL_EVERY = 5000\n",
    "MODEL_NAME = \"AGCRL_water_BINA2c\"\n",
    "MIN_REWARD = -5000\n",
    "\n",
    "\n",
    "# Create a A2C agent\n",
    "model = A2C(\"MlpPolicy\", env, learning_rate=0.003,\n",
    "          \n",
    "            verbose=1)\n",
    "\n",
    "# Create a TensorBoard summary writer\n",
    "writer = SummaryWriter(log_dir='A2C_WATERrun')\n",
    "\n",
    "\n",
    "# Continuous training loop\n",
    "ep_rewards = []\n",
    "episode = 0\n",
    "while True:\n",
    "    episode += 1\n",
    "    episode_reward = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            action, _ = model.predict(obs)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Assuming action is a scalar value\n",
    "        new_obs, reward, done, _ = env.step([action])\n",
    "\n",
    "        episode_reward += reward\n",
    "        obs = new_obs\n",
    "\n",
    "        # Log information for TensorBoard\n",
    "        writer.add_scalar('Chosen Action', action, episode)\n",
    "        writer.add_scalar('Observed State', np.mean(obs), episode)  # You can customize how you log the state\n",
    "        writer.add_scalar('Received Reward', reward, episode)\n",
    "\n",
    "        if done:\n",
    "            ep_rewards.append(episode_reward)\n",
    "\n",
    "            if epsilon > MIN_EPSILON:\n",
    "                epsilon *= EPSILON_DECAY\n",
    "                epsilon = max(MIN_EPSILON, epsilon)\n",
    "\n",
    "            if episode_reward >= MIN_REWARD and episode % SAVE_MODEL_EVERY == SAVE_MODEL_EVERY :\n",
    "                model.save(f'models/{MODEL_NAME}__ep_{episode}__reward_{float(episode_reward):.2f}.model')\n",
    "\n",
    "            model.learn(total_timesteps=1)\n",
    "\n",
    "            # Log metrics for TensorBoard\n",
    "            writer.add_scalar('Episode Reward', episode_reward, episode)\n",
    "            writer.add_scalar('Epsilon', epsilon, episode)\n",
    "\n",
    "# Close the TensorBoard writer (this line will never be reached in infinite loop)\n",
    "writer.close()\n",
    "\n",
    "# Plot results (same as before)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ep_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('Episode Rewards')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(acc_regret)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Accumulated Regret')\n",
    "plt.title('Accumulated Regret')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
