{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztjSsDRDifq_",
        "outputId": "2a5017fb-cd12-4789-fc41-41bdf110e945"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pickle\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from tensorboardX import SummaryWriter\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "# np.random.seed(42)  # You can choose any number for the seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9EpI9SHiZ3H",
        "outputId": "5309f2de-ed4c-4963-d286-c0e05aeac3df"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "class ASACRLEnv(gym.Env):\n",
        "    def __init__(self, observations, actions, action_parameter, action_space, num_intervals):\n",
        "        \"\"\"initialize action space, observation space & load data\"\"\"\n",
        "        self.action_parameter = action_parameter\n",
        "        self.num_intervals = num_intervals\n",
        "        self.action_space = gym.spaces.Box(low=action_space[0], high=action_space[1], dtype=np.float32)\n",
        "        self.observations = observations\n",
        "        self.actions = actions\n",
        "        self.index = 0\n",
        "        self.teamindex = 0\n",
        "        obs_min = np.min(np.concatenate(self.observations), axis=0)\n",
        "        obs_max = np.max(np.concatenate(self.observations), axis=0)\n",
        "\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=obs_min,\n",
        "            high=obs_max,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.curr_obs = self.observations[self.teamindex].iloc[self.index]\n",
        "        self.next_obs = self.curr_obs = self.observations[self.teamindex].iloc[self.index + 1]\n",
        "        self.curr_reward = 0\n",
        "        self.ep_reward = 0\n",
        "        self.interval = (self.action_space.high[0] - self.action_space.low[0]) / self.num_intervals\n",
        "\n",
        "    def step(self, action):\n",
        "        chosen_action = self.map_continuous_to_interval(action)\n",
        "        self.reward = self.rewardfunc(chosen_action)\n",
        "        self.ep_reward += self.reward\n",
        "        self.index += 1\n",
        "        done = False  # Set done to False by default\n",
        "\n",
        "        if self.index >= len(self.observations[self.teamindex]) - 2:\n",
        "            self.reset()\n",
        "            done = True  # Set done to True when the episode is finished\n",
        "            return self.curr_obs, self.reward, done, {}  # Return the additional info as an empty dictionary\n",
        "\n",
        "        self.curr_obs = self.observations[self.teamindex].iloc[self.index]\n",
        "        self.next_obs = self.observations[self.teamindex].iloc[self.index + 1]\n",
        "        return self.curr_obs, self.reward, done, {}\n",
        "\n",
        "    def map_continuous_to_interval(self, action):\n",
        "        interval_width = (self.action_space.high - self.action_space.low) / self.num_intervals\n",
        "        discrete_action = int((action - self.action_space.low) / interval_width)\n",
        "        return discrete_action * interval_width + self.action_space.low\n",
        "\n",
        "    def rewardfunc(self, action):\n",
        "        if action == self.estimate_closest_as(self.actions[self.teamindex][self.action_parameter][self.index]):\n",
        "            return 1000\n",
        "        else:\n",
        "            return -1 * abs(action - self.estimate_closest_as(self.actions[self.teamindex][self.action_parameter][self.index]))\n",
        "\n",
        "\n",
        "    def estimate_closest_as(self, value):\n",
        "        try:\n",
        "            closest_action_index = int(value / self.interval)\n",
        "            max_index = int((self.action_space.high[0] - self.action_space.low[0]) / self.interval)\n",
        "            closest_action_index = min(closest_action_index, max_index)\n",
        "        except ValueError:\n",
        "            closest_action_index = max_index\n",
        "        return closest_action_index\n",
        "    # def estimate_closest_as(self, value):\n",
        "    #     try:\n",
        "    #         closest_action_index = int(value / self.interval)\n",
        "    #         closest_action_index = min(closest_action_index, self.action_space.n - 1)\n",
        "    #     except:\n",
        "    #         closest_action_index = self.action_space.n - 1\n",
        "    #     return closest_action_index\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.teamindex += 1\n",
        "        if self.teamindex >= 5:\n",
        "            self.teamindex = 0\n",
        "        self.index = 0\n",
        "        self.curr_obs = self.observations[self.teamindex].iloc[self.index]\n",
        "        self.next_obs = self.observations[self.teamindex].iloc[self.index + 1]\n",
        "        self.curr_reward = 0\n",
        "        self.ep_reward = 0\n",
        "        return self.curr_obs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2P4Vis0NiUnd"
      },
      "outputs": [],
      "source": [
        "with open('/Users/faymajidelhassan/Downloads/ASAC_2023/observations.pickle', 'rb') as handle:\n",
        "    obs = pickle.load(handle)\n",
        "with open('/Users/faymajidelhassan/Downloads/ASAC_2023/actions.pickle', 'rb') as handle:\n",
        "    actions = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n7KarM76OEK",
        "outputId": "b07e8eac-34ec-4aed-ed38-d3aa880962c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: shimmy in /Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages (from shimmy) (1.23.5)\n",
            "Requirement already satisfied: gymnasium>=0.27.0 in /Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages (from shimmy) (0.28.1)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages (from gymnasium>=0.27.0->shimmy) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages (from gymnasium>=0.27.0->shimmy) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages (from gymnasium>=0.27.0->shimmy) (4.4.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages (from gymnasium>=0.27.0->shimmy) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install shimmy\n",
        "!pip install tensorboard\n",
        "# !pip install stable_baselines3\n",
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE-G2RqBDR-1",
        "outputId": "c71309a2-d2e8-4848-9343-40f82c6ca8bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/faymajidelhassan/anaconda3/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 28] No space left on device: '/var/folders/vx/1m1l_7hj50l31_jjvmr4z0hw0000gn/T/SB3-2023-08-24-20-27-52-565068'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     model\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mMODEL_NAME\u001b[39m}\u001b[39;00m\u001b[39m__ep_\u001b[39m\u001b[39m{\u001b[39;00mepisode\u001b[39m}\u001b[39;00m\u001b[39m__reward_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mfloat\u001b[39m(episode_reward)\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[39m# Update the model based on a larger number of timesteps\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)  \u001b[39m# Adjust the total_timesteps as needed\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m# Log metrics for TensorBoard\u001b[39;00m\n\u001b[1;32m     77\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mEpisode Reward\u001b[39m\u001b[39m'\u001b[39m, episode_reward, episode)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[39mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    308\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    309\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    310\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    311\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    312\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    313\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    314\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:301\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    293\u001b[0m     \u001b[39mself\u001b[39m: SelfOffPolicyAlgorithm,\n\u001b[1;32m    294\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    300\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfOffPolicyAlgorithm:\n\u001b[0;32m--> 301\u001b[0m     total_timesteps, callback \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_learn(\n\u001b[1;32m    302\u001b[0m         total_timesteps,\n\u001b[1;32m    303\u001b[0m         callback,\n\u001b[1;32m    304\u001b[0m         reset_num_timesteps,\n\u001b[1;32m    305\u001b[0m         tb_log_name,\n\u001b[1;32m    306\u001b[0m         progress_bar,\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    309\u001b[0m     callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    311\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:284\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    281\u001b[0m     pos \u001b[39m=\u001b[39m (replay_buffer\u001b[39m.\u001b[39mpos \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m replay_buffer\u001b[39m.\u001b[39mbuffer_size\n\u001b[1;32m    282\u001b[0m     replay_buffer\u001b[39m.\u001b[39mdones[pos] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_setup_learn(\n\u001b[1;32m    285\u001b[0m     total_timesteps,\n\u001b[1;32m    286\u001b[0m     callback,\n\u001b[1;32m    287\u001b[0m     reset_num_timesteps,\n\u001b[1;32m    288\u001b[0m     tb_log_name,\n\u001b[1;32m    289\u001b[0m     progress_bar,\n\u001b[1;32m    290\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:433\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39m# Configure logger's outputs if no logger was passed\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_custom_logger:\n\u001b[0;32m--> 433\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mconfigure_logger(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensorboard_log, tb_log_name, reset_num_timesteps)\n\u001b[1;32m    435\u001b[0m \u001b[39m# Create eval callback if needed\u001b[39;00m\n\u001b[1;32m    436\u001b[0m callback \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_callback(callback, progress_bar)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/stable_baselines3/common/utils.py:213\u001b[0m, in \u001b[0;36mconfigure_logger\u001b[0;34m(verbose, tensorboard_log, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39melif\u001b[39;00m verbose \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    212\u001b[0m     format_strings \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 213\u001b[0m \u001b[39mreturn\u001b[39;00m configure(save_path, format_strings\u001b[39m=\u001b[39;49mformat_strings)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/stable_baselines3/common/logger.py:650\u001b[0m, in \u001b[0;36mconfigure\u001b[0;34m(folder, format_strings)\u001b[0m\n\u001b[1;32m    648\u001b[0m     folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(tempfile\u001b[39m.\u001b[39mgettempdir(), datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39mSB3-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS-\u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    649\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(folder, \u001b[39mstr\u001b[39m)\n\u001b[0;32m--> 650\u001b[0m os\u001b[39m.\u001b[39;49mmakedirs(folder, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    652\u001b[0m log_suffix \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    653\u001b[0m \u001b[39mif\u001b[39;00m format_strings \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/var/folders/vx/1m1l_7hj50l31_jjvmr4z0hw0000gn/T/SB3-2023-08-24-20-27-52-565068'"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "from tensorboardX import SummaryWriter\n",
        "assim_rl_actionspace = np.linspace(0, 100, 21)\n",
        "\n",
        "\n",
        "# Create an instance of your custom environment\n",
        "env = ASACRLEnv(observations=obs, actions=actions, action_parameter=\"assim_sp\", action_space=assim_rl_actionspace, num_intervals=len(assim_rl_actionspace) - 1)\n",
        "\n",
        "# Wrap the environment with DummyVecEnv and VecNormalize\n",
        "env = DummyVecEnv([lambda: env])\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
        "\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1\n",
        "EPSILON_DECAY = 0.99975\n",
        "MIN_EPSILON = 0.001\n",
        "\n",
        "# Stats settings\n",
        "AGGREGATE_STATS_EVERY = 500\n",
        "SAVE_MODEL_EVERY = 100\n",
        "MODEL_NAME = \"AGCRL_ASSIM_CONT_SAC\"\n",
        "MIN_REWARD = -5000\n",
        "\n",
        "# Create a SAC agent\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, gradient_steps=-1, learning_rate=0.003, ent_coef=0)\n",
        "\n",
        "\n",
        "# Create a TensorBoard summary writer\n",
        "writer = SummaryWriter(log_dir='SAC_bin')\n",
        "\n",
        "# Continuous training loop\n",
        "ep_rewards = deque(maxlen=100)  # Stores the last 100 episode rewards\n",
        "episode = 0\n",
        "\n",
        "while True:\n",
        "    episode += 1\n",
        "    episode_reward = 0\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if np.random.random() > epsilon:\n",
        "            action, _ = model.predict(obs)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "        episode_reward += reward\n",
        "        obs = new_obs\n",
        "\n",
        "        # Log information for TensorBoard (every few steps for efficiency)\n",
        "        if episode % AGGREGATE_STATS_EVERY == 500:\n",
        "            writer.add_scalar('Chosen Action', action, episode)\n",
        "            writer.add_scalar('Observed State', np.mean(obs), episode)  # You can customize how you log the state\n",
        "            writer.add_scalar('Received Reward', reward, episode)\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if epsilon > MIN_EPSILON:\n",
        "        epsilon *= EPSILON_DECAY\n",
        "        epsilon = max(MIN_EPSILON, epsilon)\n",
        "\n",
        "    if episode_reward >= MIN_REWARD and episode % SAVE_MODEL_EVERY == 100:\n",
        "        model.save(f'{MODEL_NAME}__ep_{episode}__reward_{float(episode_reward):.2f}.model')\n",
        "\n",
        "    # Update the model based on a larger number of timesteps\n",
        "    model.learn(total_timesteps=1)  # Adjust the total_timesteps as needed\n",
        "\n",
        "    # Log metrics for TensorBoard\n",
        "    writer.add_scalar('Episode Reward', episode_reward, episode)\n",
        "    writer.add_scalar('Epsilon', epsilon, episode)\n",
        "    writer.add_scalar('Average Reward (last 100 eps)', np.mean(ep_rewards), episode)\n",
        "\n",
        "    # # Termination condition (you can adjust this)\n",
        "    # if len(ep_rewards) == 100 and np.mean(ep_rewards) > YOUR_DESIRED_REWARD_THRESHOLD:\n",
        "    #     break\n",
        "\n",
        "# Close the TensorBoard writer\n",
        "writer.close()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
